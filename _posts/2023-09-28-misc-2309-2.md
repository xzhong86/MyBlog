---
title:  "杂七杂八23-09(2)"
layout:  post
date:    2023-09-28
categories: jekyll update misc
---

# 杂七杂八23-09(2)

## 09-11
- https://neu-reality.com/2017/11/neuron-and-galaxy-networks/ , 神经元与星系网络的惊人相似性
- https://zhuanlan.zhihu.com/p/396710361 ， 高度相似的神经网络与宇宙星系

- https://cnx-software.cn/2021/05/27/rockchip-rk3566-rk3568/ , 瑞芯微RK3566和RK3568规格、数据和功能的比较

- https://www.reddit.com/r/LocalLLaMA/comments/14nf6tg/hardware_m2_ultra_192gb_mac_studio_inference/ ， 关于M2 Ultra和4090推理速度的讨论，主要围绕 llama2 65b 模型。 M2推理速度最多9 t/s，两块4090能到20+ t/s。
- https://www.reddit.com/r/LocalLLaMA/comments/157d89h/llama_v2_model_sizes/ ， 关于模型大小，基本就是参数数量乘以参数大小。如13b参数，参数类型float16，那么大小就是25GB左右。
- https://www.reddit.com/r/LocalLLaMA/comments/15lqw8n/can_cpu_llamacpp_get_close_to_over_1ts/ ， 关于CPU推理速度的讨论，以后观点说内存带宽为瓶颈的时候（多数时候），内存带宽除以模型参数大小，就等于推理的token per second。
- https://blog.csdn.net/shebao3333/article/details/131429037 ， LLaMA模型运行的消费级硬件要求

- https://www.zhihu.com/question/594159910 ， ChatGPT如何计算token数
- https://zhuanlan.zhihu.com/p/640903528 ， 支持超长上下文的大语言模型

- https://unix.stackexchange.com/questions/210982/bind-unix-program-to-specific-network-interface ， Linux下给程序制定使用的网络接口。
- https://unix.stackexchange.com/questions/255313/detecting-which-application-is-using-which-network-interface ，Linux下查看程序在使用哪个网络接口

- https://zhuanlan.zhihu.com/p/465464378 ， MLIR介绍（一）概览。 里面提到了几个基于MLIR的项目，其中就有CIRCT
- https://zhuanlan.zhihu.com/p/525705372 ， MLIR技术杂谈。（从dart语言了解到MLIR，从gitbook的替代dartbook了解到dart语言）
- https://circt.llvm.org/ ， CIRCT 基于MLIR的硬件仿真工具

- https://juejin.cn/post/7255077014494593081 ， 文心一言API使用简单教程。

## 09-13
- http://www.bimant.com/blog/code-llama-hands-on/ ， Code LLaMa快速上手。 包括少量配置代码
- https://zhuanlan.zhihu.com/p/652193073 ， Meta AI发布代码大模型Code LLaMa。 

## 09-20
- https://www.aminer.cn/research_report/64c8f9f07cb68b460f0d820a 2023年发布的25个开源大模型（至2023年7月）


## 09-19
- https://jx.huawei.com/community/comgroup/postsDetails?postId=c7b399c67ec9468293ee6b34f64e8285&noTop=true&type=freePost&previou=comments , 华为内部个人实现的一个llama cpp，能在鲲鹏920服务器上跑满带宽，支持NUMA优化。2P 128核，带宽用到260GB/s（理论290GB/s）， 70B-q8模型 推理速度能到 3.55 t/s， 34B-q8能到7.2 t/s。

## 09-26
- https://www.yanjinggongju.com/article/Matthew_9001.html , 研经工具网，一篇马太福音第一章的讲解
- https://www.yanjinggongju.com/article/uab_265 ， 圣经词语的研究方法
- https://a2z.fhl.net/gb/textual/textual32.html ， 网路带来的多译本时代与原文读经运动。 也提到圣经工具

## 09-28
- https://github.com/fxbois/web-mode ，emacs web-mode插件，支持php erb等模板各种语法高亮
- https://github.com/clarete/templatel ， 基于elisp的实现的， 借鉴 [Jinja](https://github.com/pallets/jinja/) 的模板引擎

